{"cells":[{"cell_type":"markdown","metadata":{"id":"SqovHkZacoun"},"source":["# Import libraries and initialize setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3777,"status":"ok","timestamp":1639619409475,"user":{"displayName":"Sanjar Ahmadov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHBueM86ppfmf300qWhnFwwQJNeLitSHK9E5RX=s64","userId":"06241432277290868392"},"user_tz":300},"id":"OZ__yR9WKN_R","outputId":"d14c7b28-a110-4e0c-c92a-99a36466b254"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/.shortcut-targets-by-id/1slfDnD2htWQW0hyg7pOTJg8hMvdyXWyC/Final Project\n"]}],"source":["import os \n","import pandas as pd  \n","import spacy  \n","import random\n","import statistics\n","from PIL import Image  \n","from tqdm import tqdm\n","import copy\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.nn.utils.rnn import pad_sequence  \n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.models as models\n","\n","import matplotlib.pyplot as plt\n","from nltk.translate.bleu_score import sentence_bleu\n","\n","spacy_eng = spacy.load(\"en\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd \"/content/drive/My Drive/Master's DS/Computer Vision CSCI-2271/Final Project/\"\n","torch.backends.cudnn.benchmark = True\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"Tp7MGCvncvX_"},"source":["# Define functions for loading and preparing the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4iItPCLl-DRB"},"outputs":[],"source":["class Vocabulary:\n","    def __init__(self, freq_threshold):\n","        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n","        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n","        self.freq_threshold = freq_threshold\n","\n","    def __len__(self):\n","        return len(self.itos)\n","\n","    @staticmethod\n","    def tokenizer_eng(text):\n","        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n","\n","    def build_vocabulary(self, sentence_list):\n","        frequencies = {}\n","        idx = 4\n","\n","        for sentence in sentence_list:\n","            for word in self.tokenizer_eng(sentence):\n","                if word not in frequencies:\n","                    frequencies[word] = 1\n","\n","                else:\n","                    frequencies[word] += 1\n","\n","                if frequencies[word] == self.freq_threshold:\n","                    self.stoi[word] = idx\n","                    self.itos[idx] = word\n","                    idx += 1\n","\n","    def numericalize(self, text):\n","        tokenized_text = self.tokenizer_eng(text)\n","\n","        return [\n","            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n","            for token in tokenized_text\n","        ]\n","\n","\n","class FlickrDataset(Dataset):\n","    def __init__(self, root_dir, captions_file, indices=None, transform=None, freq_threshold=5):\n","        self.root_dir = root_dir\n","        self.df = pd.read_csv(captions_file)\n","        self.transform = transform\n","\n","        # Get img, caption columns\n","        self.imgs = self.df[\"image\"]\n","        self.captions = self.df[\"caption\"]\n","\n","        # Initialize vocabulary and build vocab\n","        self.vocab = Vocabulary(freq_threshold)\n","        self.vocab.build_vocabulary(self.captions.tolist())\n","\n","        # Keep subset of data corresponding to the passed indices\n","        if indices:\n","          self.df = self.df.loc[indices].reset_index(drop=True)\n","        \n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        caption = self.captions[index]\n","        img_id = self.imgs[index]\n","        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n","        numericalized_caption += self.vocab.numericalize(caption)\n","        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n","\n","        return img, torch.tensor(numericalized_caption)\n","\n","class Padder:\n","    def __init__(self, pad_idx):\n","        self.pad_idx = pad_idx\n","\n","    def __call__(self, batch):\n","        imgs = [item[0].unsqueeze(0) for item in batch]\n","        imgs = torch.cat(imgs, dim=0)\n","        targets = [item[1] for item in batch]\n","        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n","\n","        return imgs, targets\n","\n","def split_indicies(indices_list, split_percent):\n","  random.shuffle(indices_list)\n","  cutt_off = int(len(indices_list) * split_percent)\n","  return indices_list[0 : cutt_off], indices_list[cutt_off : ]\n","\n","\n","def expand_indices(indices):\n","  res = []\n","  for i in indices:\n","      for j in range(5):\n","        res.append(i+j)\n","  return res\n","\n","\n","def get_loader_and_indices(root_folder, annotation_file, transform):\n","    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n","\n","    shirnked_indicies = [i for i in range(0, len(dataset), 5)]\n","    train_val_indices, test_indices = split_indicies(shirnked_indicies, 0.9)\n","    train_indices, validation_indices = split_indicies(train_val_indices, 0.9)\n","\n","    expanded_train_indices = expand_indices(train_indices)\n","    expanded_val_indices = expand_indices(validation_indices)\n","    expanded_test_indices = expand_indices(test_indices)\n","\n","    train_dataset = FlickrDataset(root_folder, annotation_file, indices=expanded_train_indices, transform=transform)\n","\n","    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n","    \n","    train_loader = DataLoader(\n","        dataset=train_dataset,\n","        batch_size=32,\n","        num_workers=8,\n","        shuffle=True,\n","        pin_memory=True,\n","        collate_fn=Padder(pad_idx=pad_idx),\n","    )\n","\n","    return train_loader, expanded_val_indices, expanded_test_indices, dataset"]},{"cell_type":"markdown","metadata":{"id":"09AW2pb2cMXT"},"source":["# Define Models - Encoder & Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"emwu0A3a1mSS"},"outputs":[],"source":["class Encoder_CNN(nn.Module):\n","    def __init__(self, embed_dim, dropout_rate=0.5):\n","        super(Encoder_CNN, self).__init__()\n","        self.dropout_rate = dropout_rate\n","        self.cnn = models.inception_v3(pretrained=True, aux_logits=False)\n","        self.cnn.fc = nn.Linear(self.cnn.fc.in_features, embed_dim)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.fc = nn.Linear(embed_dim, embed_dim)\n","\n","        # We do not want to mess with pretrained CNN, so just fine tune last layers\n","        for name, param in self.cnn.named_parameters():\n","            if \"fc.weight\" in name or \"fc.bias\" in name:\n","                param.requires_grad = True\n","            else:\n","                param.requires_grad = False\n","\n","    def forward(self, imgs):\n","        output = self.cnn(imgs)\n","        output = self.relu(output)\n","        output = self.dropout(output)\n","        output = self.fc(output)\n","        return output\n","\n","\n","class Decoder_LSTM(nn.Module):\n","    def __init__(self, embed_dim, hidden_dim, vocab_dim, dropout_rate=0.5):\n","        super(Decoder_LSTM, self).__init__()\n","        self.dropout_rate = dropout_rate\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.embedding = nn.Embedding(vocab_dim, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, 1)\n","        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, vocab_dim)\n","        \n","\n","    def forward(self, input, caps):\n","        word_embeds = self.embedding(caps)\n","        word_embeds = self.dropout(word_embeds)\n","        word_embeds = torch.cat((input.unsqueeze(0), word_embeds), dim=0)\n","        h, s = self.lstm(word_embeds)\n","        output = self.fc1(h)\n","        output = self.fc2(output)\n","        return output\n","\n","class EncoderToDecoder(nn.Module):\n","    def __init__(self, embed_dim, hidden_dim, vocab_dim, dropout_rate=0.5):\n","        super(EncoderToDecoder, self).__init__()\n","        self.encoder = Encoder_CNN(embed_dim, dropout_rate)\n","        self.decoder = Decoder_LSTM(embed_dim, hidden_dim, vocab_dim, dropout_rate)\n","\n","    def forward(self, imgs, caps):\n","        output = self.encoder(imgs)\n","        output = self.decoder(output, caps)\n","        return output\n"]},{"cell_type":"markdown","metadata":{"id":"PNMuc_-FJHpP"},"source":["# Load data and setup objects for training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DpTWy6l6eKBM"},"outputs":[],"source":["transform = transforms.Compose(\n","    [\n","        transforms.Resize((356, 356)),\n","        transforms.RandomCrop((299, 299)),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ]\n",")\n","\n","train_loader, validation_indices, test_indices, dataset = get_loader_and_indices(\n","    root_folder=\"/content/drive/My Drive/Master's DS/Computer Vision CSCI-2271/Final Project/data/flickr8k/Images\",\n","    annotation_file=\"/content/drive/My Drive/Master's DS/Computer Vision CSCI-2271/Final Project/data/flickr8k/captions.txt\",\n","    transform=transform\n",")\n","\n","\n","embed_dim = 512\n","dropout_rate = 0.5\n","hidden_dim = 512\n","vocab_dim = len(dataset.vocab)\n","lr = 4e-4\n","\n","model = EncoderToDecoder(embed_dim, hidden_dim, vocab_dim, dropout_rate).to(device)\n","criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n","optimizer = optim.Adam(model.parameters(), lr=lr)"]},{"cell_type":"markdown","metadata":{"id":"ZxTKqgSocdY9"},"source":["# Define training/validation and helper functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u3eyE-0vwSJY"},"outputs":[],"source":["class BLEU_SCORES(object):\n","  def __init__(self, bleu1, bleu2, bleu3, bleu4, ind_bleu1, ind_bleu2, ind_bleu3, ind_bleu4):\n","    self.bleu1 = bleu1\n","    self.bleu2 = bleu2\n","    self.bleu3 = bleu3\n","    self.bleu4 = bleu4 \n","\n","    self.ind_bleu1 = ind_bleu1\n","    self.ind_bleu2 = ind_bleu2\n","    self.ind_bleu3 = ind_bleu3\n","    self.ind_bleu4 = ind_bleu4\n","\n","  def __str__(self):\n","    return (\"Cumulative Scores: \" +\n","    str(self.bleu1) + \" \" + str(self.bleu2) +\" \"+ str(self.bleu3) +\" \"+ str(self.bleu4) +\". \" +\n","    \"Individual Scores: \" +\n","    str(self.ind_bleu1) + \" \" + str(self.ind_bleu2) + \" \" + str(self.ind_bleu3) + \" \" + str(self.ind_bleu4))\n","\n","\n","def validate(model, dataset, indicies, print_sample=False):\n","  # Returns BLEU scores for the passed dataset and indices\n","  # Randomly prints one of the datapoint as sample example if print_sample is True\n","  model.eval()\n","  total_bleu1 = 0\n","  total_bleu2 = 0\n","  total_bleu3 = 0\n","  total_bleu4 = 0\n","\n","  total_ind_bleu1 = 0\n","  total_ind_bleu2 = 0\n","  total_ind_bleu3 = 0\n","  total_ind_bleu4 = 0\n","\n","  l = len(indicies)\n","  rand_idx = random.randint(0, l-1)\n","  for i in range(0, len(indicies), 5):\n","    idx = indicies[i]\n","    img, _ = dataset[idx]\n","    candidate = generate(model, img.unsqueeze(0).to(device), dataset.vocab, 50)\n","    reference = []\n","    for offset in range(5):\n","      img2, caps = dataset[idx + offset]\n","      reference.append([dataset.vocab.itos[j] for j in caps.squeeze().tolist()])\n","\n","      if print_sample and rand_idx//5*5 == i:\n","        print(\"idx\", idx + offset)\n","        show_image(img2, \"idx\" + str(idx + offset))\n","    \n","    if print_sample and rand_idx//5*5 == i:\n","      print(\"Random example during validation\")\n","      print(\"reference\", reference)\n","      print(\"candidate\", candidate)\n","      print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n","      print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n","      print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n","      print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))\n","      print('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n","      print('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\n","      print('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\n","      print('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))\n","      print(\"\")\n","\n","    bleu1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n","    bleu2 = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\n","    bleu3 = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0))\n","    bleu4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n","\n","    ind_bleu1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n","    ind_bleu2 = sentence_bleu(reference, candidate, weights=(0, 1, 0, 0))\n","    ind_bleu3 = sentence_bleu(reference, candidate, weights=(0, 0, 1, 0))\n","    ind_bleu4 = sentence_bleu(reference, candidate, weights=(0, 0, 0, 1))\n","\n","    total_bleu1 += bleu1\n","    total_bleu2 += bleu2\n","    total_bleu3 += bleu3\n","    total_bleu4 += bleu4\n","\n","    total_ind_bleu1 += ind_bleu1\n","    total_ind_bleu2 += ind_bleu2\n","    total_ind_bleu3 += ind_bleu3\n","    total_ind_bleu4 += ind_bleu4\n","\n","  # Divide l by 5 to get number of examples\n","  l = l//5\n","  model.train()\n","  return BLEU_SCORES(total_bleu1/l, total_bleu2/l, total_bleu3/l, total_bleu4/l, total_ind_bleu1/l, total_ind_bleu2/l, total_ind_bleu3/l, total_ind_bleu4/l)\n","\n","def show_image(inp, title=None):\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)\n","\n","def print_random_example(model, dataset, indicies):\n","    idx = random.choice(indicies)\n","    idx = idx//5 * 5\n","    model.eval()\n","    img, caps = dataset[idx]\n","    show_image(img, \"Image\")\n","    for i in range(5):\n","      actual = \" \".join([dataset.vocab.itos[j] for j in dataset[idx+i][1].squeeze().tolist()])\n","      print(\"Example idx:\", idx, \"TARGET :\", actual)\n","    print(\"Example OUTPUT:\", \" \".join(generate(model, img.unsqueeze(0).to(device), dataset.vocab, 50)))\n","    model.train()\n","\n","\n","def generate(model, image, vocab, max_len):\n","    res = []\n","\n","    with torch.no_grad():\n","        x = model.encoder(image).unsqueeze(0)\n","        states = None\n","        for i in range(max_len):\n","            hiddens, states = model.decoder.lstm(x, states)\n","            output = model.decoder.fc1(hiddens.squeeze(0))\n","            output = model.decoder.fc2(output)\n","            pred = output.argmax(1)\n","            x = model.decoder.embedding(pred).unsqueeze(0)\n","\n","            res.append(pred.item())\n","\n","            if vocab.itos[pred.item()] == \"<EOS>\":\n","                break\n","\n","    return [vocab.itos[idx] for idx in res]\n","\n","\n","def train(model, criterion, opt, epochs, train_loader, validation_indices, test_indices, dataset, validate_interval):\n","    BELUs_list = []\n","\n","    avg_losses = []\n","    best_model = None\n","    best_bleu4 = 0.0\n","    for epoch in range(epochs):\n","        print_random_example(model, dataset, validation_indices)\n","        print_random_example(model, dataset, validation_indices)\n","        print_random_example(model, dataset, validation_indices)\n","        model.train()\n","\n","        total_loss = 0\n","        for i, (imgs, caps) in tqdm(enumerate(train_loader), total=len(train_loader), leave=False):\n","            caps = caps.to(device)\n","            imgs = imgs.to(device)\n","            output = model(imgs, caps[:-1])\n","\n","            loss = criterion(output.reshape(-1, output.shape[2]), caps.reshape(-1))\n","            total_loss += loss.item()\n","\n","            if i % validate_interval == 0:\n","              BLEUs = validate(model, dataset, validation_indices)\n","              avg_loss = total_loss/(i+1)\n","              total_loss = 0\n","\n","\n","              BELUs_list.append((BLEUs.bleu1, BLEUs.bleu4))\n","              avg_losses.append(avg_loss)\n","              if best_bleu4 < BLEUs.bleu4:\n","                  best_bleu4 = BLEUs.bleu4\n","                  best_model = copy.deepcopy(model)\n","\n","              print(\"BLEU 1\", BLEUs.bleu1, \"BLEU 4\", BLEUs.bleu4, \"LOSS\", loss.item())\n","\n","            opt.zero_grad()\n","            loss.backward(loss)\n","            opt.step()\n","\n","        print(\"EPOCH:\", epoch, \"LOSS:\", avg_loss, \"BLEU 1-4\", BLEUs)\n","\n","    return BELUs_list, avg_losses, best_model\n"]},{"cell_type":"markdown","metadata":{"id":"P3LQsxnCcXQp"},"source":["# Run training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Q4Iez_T2pschq7n2aJi9kuyjjHIy24fh"},"executionInfo":{"elapsed":457865,"status":"ok","timestamp":1639630711812,"user":{"displayName":"Sanjar Ahmadov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHBueM86ppfmf300qWhnFwwQJNeLitSHK9E5RX=s64","userId":"06241432277290868392"},"user_tz":300},"id":"6MTLaUyafIlR","outputId":"86b3a888-a54d-4d59-a085-5fa098382ca9"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["BELUs_list, avg_losses, best_model = train(model, criterion, optimizer, 15, train_loader, validation_indices, test_indices, dataset, 100)\n","print(BELUs_list)\n","print(avg_losses)"]},{"cell_type":"markdown","metadata":{"id":"Zj_Skv3XHjA3"},"source":["# Get BLEU scores for Test data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OVk4N7-kE9nU","outputId":"ede10192-35e9-428d-c94d-aee8279ff5a0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 3-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 4-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"]}],"source":["BLEUs = validate(best_model, dataset, test_indices)\n","print(\"BLEU 1\", BLEUs.bleu1, \"BLEU 4\", BLEUs.bleu4)"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"Image Captioning.ipynb","provenance":[{"file_id":"12udK7NizLGTaKEmoNOmSk92JmBq42w3S","timestamp":1638507608802}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}